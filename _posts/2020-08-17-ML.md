[Why batch size and what is mini-bach](https://towardsdatascience.com/how-to-break-gpu-memory-boundaries-even-with-large-batch-sizes-7a9c27a400ce)

[What is gradient accumulation?](https://towardsdatascience.com/what-is-gradient-accumulation-in-deep-learning-ec034122cfa)

[Batch vs Epoch vs Iteration](https://machinelearningmastery.com/difference-between-a-batch-and-an-epoch/)

[Batch vs Epoch vs Iteration 2](https://towardsdatascience.com/epoch-vs-iterations-vs-batch-size-4dfb9c7ce9c9)

[Pytorch distributed traning](https://pytorch.org/tutorials/beginner/aws_distributed_training_tutorial.html), [Sample code](https://github.com/pytorch/examples/blob/master/imagenet/main.py)

[Pytorch multi-gpu(data parallel)](https://pytorch.org/tutorials/beginner/former_torchies/parallelism_tutorial.html)

[Pytorch distributed applications](https://pytorch.org/tutorials/intermediate/dist_tuto.html)
* [P2P communication](https://pytorch.org/tutorials/intermediate/dist_tuto.html#point-to-point-communication)
* [Collective Communication](https://pytorch.org/tutorials/intermediate/dist_tuto.html#point-to-point-communication)

[What is ring-reduce/all-reduce](https://pytorch.org/tutorials/intermediate/dist_tuto.html)

[Backends: nccl, gloo, mpi](https://pytorch.org/docs/stable/distributed.html#backends)


### Greate online [IDE](https://medium.com/gitpod/gitpod-gitpod-online-ide-for-github-6296b907a886)
