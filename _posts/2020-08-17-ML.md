### Tensorflow
TensorFlow is composed of two parts, a library for defining computational graphs and a runtime for executing these graphs, that’s the **Graph** and **Session**

[Understand TensorFlow graph](https://medium.com/@d3lm/understand-tensorflow-by-mimicking-its-api-from-scratch-faa55787170d)
* Variables: Think of TensorFlow variables like normal variables in our computer programs. A variable can be modified at any point in time, but the difference is that they have to be initialized before running the graph in a session. They represent changeable parameters within the graph. A good example for variables would be the weights or biases in a neural network.
* Placeholders: A placeholder allows us to feed data into the graph from outside and unlike variables they don’t need to be initialized. Placeholders simply define the shape and the data type. We can think of placeholders as empty nodes in the graph where the value is provided later on. They are typically used for feeding in inputs and labels.
* Constants: Parameters that cannot be changed.
* Operations: Operations represent nodes in the graph that perform computations on Tensors.
* Graph: A graph is like a central hub that connects all the variables, placeholders, constants to operations.
* Session: A session creates a runtime in which operations are executed and Tensors are evaluated. It also allocates memory and holds the values of intermediate results and variables.

#### sample tf graph
* graph defination
```python
import tensorflow as tf

with tf.Session() as sess:
    # build graph to session default graph
    a = tf.constant(15, name="a")
    b = tf.constant(25, name="b")
    mul = tf.multiply(a, b, name="multiply")
    summation = tf.add(a, b, name="add")
    out = tf.divide(mul, summation, name="divide")

    # execute graph through sess
    o = sess.run(out)
    print(o)

    # save logs for tensorboard
    fw = tf.summary.FileWriter('./logs', sess.graph)
```

* start [tensorboard](https://github.com/tensorflow/tensorboard)
  1. tensorboard --logdir ./logs
  2. navigate to localhost:6006



### Distributed traning

[Why batch size and what is mini-bach](https://towardsdatascience.com/how-to-break-gpu-memory-boundaries-even-with-large-batch-sizes-7a9c27a400ce)

[What is gradient accumulation?](https://towardsdatascience.com/what-is-gradient-accumulation-in-deep-learning-ec034122cfa)

[Batch vs Epoch vs Iteration](https://machinelearningmastery.com/difference-between-a-batch-and-an-epoch/)

[Batch vs Epoch vs Iteration 2](https://towardsdatascience.com/epoch-vs-iterations-vs-batch-size-4dfb9c7ce9c9)

[Pytorch distributed traning](https://pytorch.org/tutorials/beginner/aws_distributed_training_tutorial.html), [Sample code](https://github.com/pytorch/examples/blob/master/imagenet/main.py)

[Pytorch multi-gpu(data parallel)](https://pytorch.org/tutorials/beginner/former_torchies/parallelism_tutorial.html)

[Pytorch distributed applications](https://pytorch.org/tutorials/intermediate/dist_tuto.html)
* [P2P communication](https://pytorch.org/tutorials/intermediate/dist_tuto.html#point-to-point-communication)
* [Collective Communication](https://pytorch.org/tutorials/intermediate/dist_tuto.html#point-to-point-communication)

As opposed to point-to-point communcation, collectives allow for communication patterns across all processes in a **group**. A group is a subset of all our processes. To create a group, we can pass a list of ranks to dist.new_group(group). By default, collectives are executed on the all processes, also known as the **world**.

  * Scatter:dist.scatter(tensor, src, scatter_list, group): Copies the ith tensor scatter_list[i] to the ith process.![Scatter](https://pytorch.org/tutorials/_images/scatter.png)
  * Gather:dist.gather(tensor, dst, gather_list, group): Copies tensor from all processes in dst.![Gather](https://pytorch.org/tutorials/_images/gather.png)
  * Reduce:dist.reduce(tensor, dst, op, group): Applies op to all tensor and stores the result in dst.![Reduce](https://pytorch.org/tutorials/_images/reduce.png)
    * dist.reduce_op.SUM,
    * dist.reduce_op.PRODUCT,
    * dist.reduce_op.MAX,
    * dist.reduce_op.MIN
  * All Reduce:dist.all_reduce(tensor, op, group): Same as reduce, but the result is stored in all processes.![All Reduce](https://pytorch.org/tutorials/_images/all_reduce.png)
  * Broadcast:dist.broadcast(tensor, src, group): Copies tensor from src to all other processes.
  ![Broadcast](https://pytorch.org/tutorials/_images/broadcast.png)
  * All Gather:dist.all_gather(tensor_list, tensor, group): Copies tensor from all processes to tensor_list, on all processes.![All Gather](https://pytorch.org/tutorials/_images/all_gather.png)
  * dist.barrier(group): block all processes in group until each one has entered this function.

[What is ring-reduce/all-reduce](https://pytorch.org/tutorials/intermediate/dist_tuto.html)


Pytorch traning example
* [Single node](https://github.com/pytorch/examples/blob/master/mnist/main.py)
* [Distributed traning](https://pytorch.org/tutorials/intermediate/dist_tuto.html#distributed-training)


[Backends: nccl, gloo, mpi](https://pytorch.org/docs/stable/distributed.html#backends)

[when to use which backend](https://pytorch.org/docs/stable/distributed.html#which-backend-to-use)

[Pytorch distributed backend](https://pytorch.org/tutorials/intermediate/dist_tuto.html#communication-backends)
* [Gloo](https://github.com/facebookincubator/gloo)
* Message Passing Interface (MPI)
    * [Open-MPI](https://www.open-mpi.org/)
    * [MVAPICH2](http://mvapich.cse.ohio-state.edu/)
    * [Intel MPI](https://software.intel.com/en-us/intel-mpi-library)
* [NCCL](https://github.com/nvidia/nccl) 

[Pytorch distributed initialization method](https://pytorch.org/tutorials/intermediate/dist_tuto.html#initialization-methods)
* MASTER_PORT: A free port on the machine that will host the process with rank 0.
* MASTER_ADDR: IP address of the machine that will host the process with rank 0.
* WORLD_SIZE: The total number of processes, so that the master knows how many workers to wait for.
* RANK: Rank of each process, so they will know whether it is the master of a worker.

### Greate online [IDE](https://medium.com/gitpod/gitpod-gitpod-online-ide-for-github-6296b907a886)
