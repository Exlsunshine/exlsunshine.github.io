[Why batch size and what is mini-bach](https://towardsdatascience.com/how-to-break-gpu-memory-boundaries-even-with-large-batch-sizes-7a9c27a400ce)

[What is gradient accumulation?](https://towardsdatascience.com/what-is-gradient-accumulation-in-deep-learning-ec034122cfa)

[Batch vs Epoch vs Iteration](https://machinelearningmastery.com/difference-between-a-batch-and-an-epoch/)

[Batch vs Epoch vs Iteration 2](https://towardsdatascience.com/epoch-vs-iterations-vs-batch-size-4dfb9c7ce9c9)

[Pytorch distributed traning](https://pytorch.org/tutorials/beginner/aws_distributed_training_tutorial.html), [Sample code](https://github.com/pytorch/examples/blob/master/imagenet/main.py)

[Pytorch multi-gpu(data parallel)](https://pytorch.org/tutorials/beginner/former_torchies/parallelism_tutorial.html)

[Pytorch distributed applications](https://pytorch.org/tutorials/intermediate/dist_tuto.html)
* [P2P communication](https://pytorch.org/tutorials/intermediate/dist_tuto.html#point-to-point-communication)
* [Collective Communication](https://pytorch.org/tutorials/intermediate/dist_tuto.html#point-to-point-communication)

As opposed to point-to-point communcation, collectives allow for communication patterns across all processes in a **group**. A group is a subset of all our processes. To create a group, we can pass a list of ranks to dist.new_group(group). By default, collectives are executed on the all processes, also known as the **world**.

  * Scatter:dist.scatter(tensor, src, scatter_list, group): Copies the ith tensor scatter_list[i] to the ith process.![Scatter](https://pytorch.org/tutorials/_images/scatter.png)
  * Gather:dist.gather(tensor, dst, gather_list, group): Copies tensor from all processes in dst.![Gather](https://pytorch.org/tutorials/_images/gather.png)
  * Reduce:dist.reduce(tensor, dst, op, group): Applies op to all tensor and stores the result in dst.![Reduce](https://pytorch.org/tutorials/_images/reduce.png)
    * dist.reduce_op.SUM,
    * dist.reduce_op.PRODUCT,
    * dist.reduce_op.MAX,
    * dist.reduce_op.MIN
  * All Reduce:dist.all_reduce(tensor, op, group): Same as reduce, but the result is stored in all processes.![All Reduce](https://pytorch.org/tutorials/_images/all_reduce.png)
  * Broadcast:dist.broadcast(tensor, src, group): Copies tensor from src to all other processes.
  ![Broadcast](https://pytorch.org/tutorials/_images/broadcast.png)
  * All Gather:dist.all_gather(tensor_list, tensor, group): Copies tensor from all processes to tensor_list, on all processes.![All Gather](https://pytorch.org/tutorials/_images/all_gather.png)
  * dist.barrier(group): block all processes in group until each one has entered this function.

[What is ring-reduce/all-reduce](https://pytorch.org/tutorials/intermediate/dist_tuto.html)

[Backends: nccl, gloo, mpi](https://pytorch.org/docs/stable/distributed.html#backends)

[Pytorch traning example(mnist)](https://github.com/pytorch/examples/blob/master/mnist/main.py)

### Greate online [IDE](https://medium.com/gitpod/gitpod-gitpod-online-ide-for-github-6296b907a886)
